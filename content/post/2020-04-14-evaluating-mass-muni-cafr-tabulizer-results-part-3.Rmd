---
title: Evaluating Mass Muni CAFR Tabulizer Results - Part 3
author: David Lucey
date: '2020-04-14'
slug: evaluating-mass-muni-cafr-tabulizer-results-part-3
categories: ["R", "Code-Oriented"]
tags: ["pdf", "pdftools", "tabulizer", "textract"]
draft: true
output:
  html_document:
    code_folding: yes
---


```{r 'set-up', message=FALSE, warning=FALSE, include=FALSE}

library(rlist)
library(data.table)
library(stringr)
library(pdftools)
library(readxl)

```


# Introduction

This post is a continuation [Tabulizer and pdftools Together as Super-powers - Part 2](https://redwallanalytics.com/2020/04/06/tabulizer-and-pdftools-togeteher-as-super-powers-part-2/) of Although the full process used to extract data from all our 150 PDFs will not shown, to review the steps that Redwall followed for the portion using pdftools and tabulizer:

  1. Load all the pdf_data (metadata) for every page of the 150 PDFs into a list of lists with the metadata data.frame from pdftool's pdf_data.
  2. Filter the nested list to keep only the key tables (ie:  Statement of Activities, Balance Sheet, Statement of Net Position) using regular expression matching of sub-elements.
  3. Find the coordinates for the key Tabulizer area parameters using regular expressions and the pdf_data metadata.
  4. Extract those tables with Tabulizer into data.frames. 
  5. Clean up the raw output especially column headers.
  6. Extract out key fields (ie: Assigned Net Balance, Unassigned Net Balance and Total Expenditures).
  7. Compare to manually extracted data from Marc Joffe.
  
Please see our [Github](https://github.com/luceydav/pdf_cafr_parse/blob/master) for the code. In this post, we will evaluate how we did with with this first method.


```{r 'load-compare', echo=TRUE, message=FALSE, warning=FALSE}

# Load comparison of scraped vs Reason spreadsheet
mass_compare <- 
  readxl::read_excel("~/Desktop/David/Projects/pdf_cafr_parse/mass_compare.xls", 
    col_types = c("skip", "text", "numeric", 
        "numeric", "numeric", "numeric", 
        "numeric", "numeric", "numeric", 
        "numeric", "numeric", "numeric", 
        "numeric", "numeric", "numeric", 
        "numeric", "numeric"))
mass_compare <- setDT(mass_compare)


```



# Tabulizer Results Compared to Manual Spreadsheet

In the code below, we show the PDFs where all fields did not match the manually-compiled spreadsheet. It is encouraging that there are only 6, Boston didn't match only because its CAFR numbers were rounded to the nearest thousand, while our reference spreadsheet was not. We also discovered that our CAFR library didn't have a 2018 CAFR in some cases, so that was the reason some didn't return any scraped results. In addition, some CAFR's are released as images, which don't work with the tabulizer OCR, but might work in AWS Textract.


```{r 'all-missing'}

# Filter diff != 0 (cases where there wasn't a match)
diff <- 
  names(mass_compare)[str_detect(names(mass_compare), "diff_")]

# All missing
all_missing <- mass_compare[, 
  problem := as.logical(apply(.SD, 1, function(x) all(x != 0))), 
  .SDcols = diff][
    ][problem == 1]$muni

all_missing

```


The next table shows cases where one or more variable was not being extracted properly (difference not equal to zero). Of the 660 potential items in the spreadsheets which were successfully parsed, 33 did not match. 95% accuracy seems pretty good at first glance, but an analyst probably couldn't rely on this without a manual checking process. Another possible objective would be to know which formats would have a higher likelihood of failure, and just check those. For example, some tables run over onto a second page, and others have sections with larger indentations. In the next step, we will slice the problematic tables out of the relevant PDF, and run these through Textract to see what happens.


```{r 'bad-elements', echo=FALSE, message=FALSE, warning=FALSE}

# Find cases where at least one scraped item didn't match
any_missing <- mass_compare[, 
  problem := as.logical(apply(.SD, 1, function(x) any(x != 0))), 
  .SDcols = diff][
    ][problem == 1]$muni

# Bad cases where not all are missing
bad_elements <- setdiff(any_missing,all_missing)

# Show bad cases
DT::datatable(mass_compare[muni %in% bad_elements, 
             .SD, .SDcols=patterns("diff|muni")])

```


# Conclusion

The combination of pdftools and Tabula gave encouraging results, and we are confident that further fine tuning could further reduce the error rate. We also have some ideas about which tables fail most often, so the risk of an unexpected mismatch might be mitigated by manual intervention or getting a back up from Textract in these cases. In the next post, we use the AWS Textract SDK to use their machine learning based solution to see how their solution does.
