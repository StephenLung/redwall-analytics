---
title: Exploring Big MT Cars with Python datatable and plotnine
author: David Lucey
date: '2020-05-07'
slug: exploring-big-mt-cars-with-python-datatable-and-plotnine
categories: ["R", "Code-Oriented"]
tags: ["data.table", "datatable", "plotnine"]
output:
  html_document:
    code_folding: 'hide'
---




```{r 'setup', message=FALSE, warning=FALSE, cache=FALSE, include=TRUE}
# R Libraries
library("reticulate")
library("skimr")

knitr::opts_chunk$set(
  fig.width = 15,
  fig.height = 8,
  out.width = '100%')

```


```{r 'reticulate', echo=TRUE, message=FALSE, warning=FALSE}
# Choose Python 3.7 miniconda
reticulate::use_condaenv(
  condaenv = "datatable",
  conda = "/Users/davidlucey/opt/anaconda3/bin/conda",
  required = TRUE
  )

```


```{r 'install-conda', message=FALSE, warning=FALSE, eval = FALSE}
# Install Python packages
lapply(c("plotnine", "datatable", "re"), function(package) {
       conda_install("datatable", package, pip = TRUE)
})

```


```{python 'python-setup'}
# Python libraries
import datatable as dt
import numpy as np
import re

```

# Introduction

As mentioned in [Parsing Mass Municipal PDF CAFRs with Tabulizer, pdftools and AWS Textract - Part 1](https://redwallanalytics.com/2020/03/31/parsing-mass-municipal-pdf-cafrs-with-tabulizer-pdftools-and-aws-textract-part-1/), Redwall Analytics is going through this year, and solving problems previously out of reach. It is also doing this by combining tools as we did with `pdftools` and `tabulizer` in the previous series. Although we have worked with some Python, we have been hoping to bridge our way into the language with the Python implementations of two of our favorite R packages, `data.table` and `ggplot2`. 

The Python `datatable` was launched by h2o two years ago, and feels very similar to the R version with some small syntax differences and some important pieces still missing. We could only find a handful of posts showing how to use datatable, and those felt like they were not written by regular users of R `data.table`. We use data.table every day and love the speed and consise syntax, so will discuss datatable from that perspective. `plotnine` feels more seemless with `ggplot2`, another library we use every day, with a few problems formatting plots in R Studio. 


# EPA's Big MT Dataset

To make it interesting, we will use the [Tidy Tuesday Big MT Cars]("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-15/big_epa_cars.csv) with 36 years of 42,230 new US car models. The data dictionary with 83 variables describing each annual new car model is found [here](https://www.fueleconomy.gov/feg/ws/index.shtml#fuelType1). Everyone loves cars and reminiscing about historical models. We have naturally been curious about this dataset, but in closer analysis, have discovered that there are several unfortunate missing pieces. When we have modeled `mtcars`, weight (`wt`) and horsepower (`hp`), and their interaction, have been most informative for predicting `mpg`. It would have been interesting to look at the evolution of these coefficients over time, but theses variables are not available. In addition, it is hard to get a sense of fleet mileage without the total annual sales of each new car model. Because of this, it is impossible to know the evolution of more fuel efficient electric vehicles relative to more fuel-hungry model sales. It is difficult to understand why these variables are not included because they took quite a lot of trouble to compile the data, are that information must be available to the EPA. While the dataset is still of interest, including these fields would improve it considerably!


```{python 'load-big-mt'}
# Load vehicles
#https://www.fueleconomy.gov/feg/ws/index.shtml#fuelType1
#big_mt_cars <- fread("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-15/big_epa_cars.csv")
big_mt = dt.fread("~/Downloads/vehicles.csv")

# Dimensions
big_mt.shape

```

Some of the most important variables are shown in the code below, and even these need significant cleaning. The first difference with R `data.table` is shown below with the `year_filter`. Using the filter in `i` (the first slot), the 1335 2019 models are shown below. Unlike R data.table, we refer to the `year` variable outside of the frame in an expression, and then call it within `i` in the frame. The columns can be selected within `()` or `[]` in `j` (the second slot) as shown below. We will discuss the `dt.f` syntax, which also was confusing at first, below. We thought there might be duplicate rows, but there are no duplicated() or unique() functions yet in `datatable` [How to find unique values for a field in Pydatatable Data Frame](https://stackoverflow.com/questions/61578175/how-to-find-unique-values-for-a-field-in-pydatatable-data-frame), so to do this, identifying variables would have to be grouped, counted and filtered for equal to 1. In an unfamiliar dataset, there might still leave uncertainty if all the duplicates had been appropriately captured. After searching for ways to do it in `datatable`, we had to pipe over to `pandas` to verify that there were no duplicates, but hope this will be added in the future.


```{python 'first-look', echo=TRUE}
# Key variables for year 2019
year_filter = (dt.f.year == 2019)
print(big_mt[year_filter, (dt.f.year, dt.f.make, dt.f.model, dt.f.trany, dt.f.evMotor, dt.f.VClass)])

```

We struggled with column transformations, like separating to tidy variables, which still seems easier with `data.table`. To start with, `trany` has both the transmission-type and gear-speed variable within it. It may be that our Python ability is lacking, but the need to iterate over the nested list or tuples structure of `datatable` variables wouldn't allow us to use regex patterns without drilling down on list comprehensions. We found that the easiest thing to do in these cases was to send the data to `pandas`, and split and replace as shown in the first two steps below. An easier method from within the `datatable` frame would be appreciated, because this is a frequent challenge when data cleaning.

In the third line of code, we felt like we were using an R `data.table`. The `{}` is used to create the new variable without affecting the other variables, similar to how `:=` is be used within `data.table` (though the result still has to be assigned). Notice that we had to refer to variables with the `datatable` non-standard evaluation function (ie: `dt.f.trans` and `dt.f.speed`) if we wanted to avoid having to include quotation marks around the variable. In Python `datatable`, an extra step is required to `export_names()` to skip `dt.f` or the quotation marks (as we will show further down). We also have to call other functions by referring to the package short name `dt`, as we did with `dt.count()` and `dt.by()` below (even when working within the frame). 

With `datatable`, we frequently forgot the need to include `:` in `i` or `j` even if we were not specifically giving instructions for those slots. By default, `datatable` needs to specifically be instructed unlike `data.table` which assumes all rows or columns in `i` and `j` by default. Also, `dt.sort` and several other operations are done in the third slot within the frame (see print statement below), unlike in `data.table` where they are performed in `i` of the next frame. A last difference which we discovered in SO but not the documentation was the option to use the `\` operator to chain a frame to the next line (also shown in the print statement below).


```{python 'split-trany', echo=TRUE}
# Move to pandas to split "trany" variable
big_mt[['trans', 'speed']] = big_mt.to_pandas().pop('trany').str.split('\s', 1, expand=True)

# Again move to pandas to remove non digit chars
big_mt['speed'] = big_mt.to_pandas().pop('speed').str.replace('\D*', '')

# Summarize percent of instances by transmission and speed
print(big_mt[:, {'percent' : dt.count()/big_mt.nrows}, dt.by(dt.f.trans, dt.f.speed)]\
            [0:10, : ,dt.sort(-dt.f.percent)])

```

We wanted to create a boolean variable to denote if a vehicle was electric or not. In order to do this, we had to pipe our `evMotor` evaluation from `j` out to pandas in order to form a data structure we could assign back to the newly created `is_ev` variable (if the row identifying the battery was blank or not). We also had difficulty with row filtering at first, but got more used it over time. Above we showed how to filter with an expression set up outside the frame, but below `(dt.f.is_ev == 1)` is used to filter in `i` within the frame. Without the parentheses, the expression won't work whether it is set up within or outside the frame. We sometimes also had difficulty using multiple expressions to filter a frame, or if we wanted to filter, and then perform operations in `j` of the same frame. In the table below, we show the number of electric vehicles rising from 3 in 1998 to 149 this year.



```{python 'flag-ev', echo=TRUE}
# Use pandas to create is_ev using expression from dt
big_mt['is_ev'] = big_mt[:, dt.f.evMotor != '' ].to_pandas()

# Summarize number of ev models by year
print(big_mt[:, dt.count(), dt.by(dt.f.is_ev, dt.f.year)][(dt.f.is_ev == 1), ('year', 'count')])

```

Next, we wanted to extract wheel-drive (2WD, AWD, 4WD, etc) and engine type (ie: V4, V6, etc) from `model`. This would require to filter where for example a row had '2WD' or '4WD' in model, and here we sorely missed the `data.table` `%in%` and `%chin%` operators, and it seems that we are not alone. This [SO post](https://stackoverflow.com/questions/61494957/how-to-filter-observations-for-the-multiple-values-passed-in-the-i-expression-of) identifies similar challenges we encountered when filtering on a group of instead of a straight equality. The suggested solution iterating over all the possible combinations is sizeable friction for us, so hopefully the requested feature will be implemented soon. We see the total count in the table, and also that we are still missing engine information in most cases. 

```{python 'wheel', echo=TRUE}
# Convert model to str and split into wheel and engine
models = [tup[0] for tup in big_mt[:, 'model'].to_tuples()]
#np.where(re.search(r'\dWD|AWD', models), re.findall(r'\dWD|AWD', models), 'nan')
big_mt[:, 'wheel'] = dt.Frame([re.findall(r'\dWD|AWD', x)[0] if re.search(r'\dWD|AWD', x) is not None else 'nan' for x in models])
big_mt[:, 'engine'] = dt.Frame([re.findall(r'V\d', x)[0] if re.search(r'V\d', x) is not None else 'nan' for x in models])

# Fix problem notations
big_mt.replace("\dwd", "\dWD")

# Summarize total count for all years
cols = ['make', 'model', 'drive', 'engine', 'cylinders', 'wheel', 'tCharger', 'sCharger']
print(big_mt[(dt.f.wheel != 'nan'), cols]\
            [:, dt.count(), dt.by(dt.f.engine, dt.f.wheel, dt.f.cylinders)]\
            [0:13,:, dt.sort(-dt.f.count)])

```


There was no such thing as an SUVs or AWD back in the 80's, and we remember the V8 Oldsmobile's and Cadillac's, so were curious how these models evolved over time. `datatable` doesn't yet have dcast() or melt(), so we had to pipe these out `to_pandas` and then use `pivot_table()`. Its likely that a lot of the nan's were just 2WD before there was a need to specify in the model name. We can see the first AWD models starting in the late 80s, and the number of 8-cylinder cars fall by half. There are are a lot fewer annual new car models now than in the 80s, but were surprised how many fewer 4-cylinders.


```{python 'models-over-time', echo=TRUE}
# Summarize by year again having to move to pandas to pivot
print(big_mt[:, dt.count(), dt.by(dt.f.wheel, dt.f.year)].to_pandas().pivot_table(index='wheel', columns='year', values='count'))

print(big_mt[:, dt.count(), dt.by(dt.f.cylinders, dt.f.year)].to_pandas().pivot_table(index='cylinders', columns='year', values='count'))
```



```{python 'cleaner-function', include=FALSE}
# Control flow statement used to collapse VClass levels in clean-vclass chunk below
def collapse_vclass(type):
  if type in ['Compact Cars', 'Two Seaters', 'Subcompact Cars', 'Minicompact Cars', 'Small Station Wagons']:
      type = 'Small Car'
  elif type in ['Midsize Cars', 'Midsize Station Wagons']:
      type = 'Midsize Car'
  elif type in ['Midsize-Large Station Wagons', 'Large Cars']:
      type = 'Large Car'
  elif type in ['Special Purpose Vehicle', 'Special Purpose Vehicles']:
      type = 'Special Purpose Vehicle'
  elif type in ['Vans Passenger', 'Vans, Passenger Type', 'Vans, Cargo Type', 'Vans']:
      type = 'Vans'
  elif type in ['Sport Utility Vehicle', 'Standard Sport Utility Vehicle']:
      type = 'Sport Utility Vehicle'
  elif type in ['Small Pickup Trucks', 'Small Sport Utility Vehicle']:
      type = 'Small Pickup and SUV'
  return type
  
```


`VClass`, which had 35 levels often referring to similar vehicles, also needed transformation. Again, these kinds of operations were where we struggled and missed `data.table`. Even in R `data.table`, we have been keenly awaiting the implemention of `fcase`, which is expected to channel `dplyr` `case_when()` functionality for nested control-flow statements. We made a separate 16-line function to check the factor levels (not shown). In the first line below, we created the `vclasses` list to drill down on the `VClass` tuples elements as strings. In the second line, we had to iterate over the resulting strings to extract wheel-drive with a list-comprehension. In order to put the list back into the dt as a column, we had to call `dt.Frame()`, and another expression to merge our two `wheel` columns, which would have been a simple `fcoalesce` or `fifelse()` within the frame of the R `data.table`. We then same complicated steps to remove wheel-drive from the remaining `VClass`. Again, if anybody knows of more succinct ways of accomplishing these operations, we would be grateful to know.


```{python 'clean-vclass', echo=TRUE}
# Take wheel info out of VClass and merge with wheel variable where missing
vclasses = [tup[0] for tup in big_mt[:, 'VClass'].to_tuples()]
big_mt['wheel1'] = dt.Frame([re.findall(r'\dWD|\dwd', x)[0] if re.search(r'WD$|wd$', x) is not None else 'nan' for x in vclasses])
big_mt['wheel'] = dt.Frame(big_mt[:, dt.f.wheel if not 'nan' else dt.f.wheel1])

# Clean up vehicle type from VClass
big_mt['VClass'] = dt.Frame([re.sub('\s\dWD$|\/\dwd$|\s\-\s\dWD$', '', x) if re.search(r'WD$|wd$', x) is not None else x for x in vclasses])
big_mt['VClass'] = dt.Frame([collapse_vclass(line[0]) for line in big_mt[:, 'VClass'].to_tuples()])

# Show final VClass types
print(big_mt[:, dt.f.VClass, dt.by(dt.f.VClass)]\
            [0:11, 'VClass'].to_numpy())

```



```{python 'engine=wheel-result', echo=TRUE}
cols = ['make','model','year', 'VClass', 'engine', 'wheel']
print(big_mt[((dt.f.engine != 'nan') & (dt.f.wheel != 'nan')), cols])
```


We show the result of our efforts to clean `model` and `VClass` below. After all that, we see we only have 218 rows where both `engine` and `wheel`are not 'nan', so if we hoped to use these variables in a future modeling step, we will have to look for other ideas.  Below, we show how to `export_names()` in order to specify those variables as available for non-standard evaluation. We waited while we were transforming variables above to do this, but maybe it would make sense to do it along the way as new variables are created. This happens automatically in `data.table`, and is an added layer of inconvenience. 


```{python 'export-names', echo=TRUE}
# List of cols to keep
cols = ['make', 
        'model', 
        'year', 
        'city08', 
        'highway08', 
        'comb08', 
        'VClass', 
        'drive',
        'fuelType1', 
        'hlv', 
        'hpv', 
        'cylinders', 
        'displ',
        'trans', 
        'speed',
        'wheel',
        'is_ev',
        'evMotor', 
        'guzzler',
        'tCharger',
        'sCharger']
        
# Export_names of key variables so can be called without dt.f prefix. (Note: only works if cols selected)
make, model, year, city08, highway08, comb08, VClass, drive, fuelType1, hlv, hpv, cylinders, displ, trans, speed, wheel, is_ev, evMotor, guzzler, tCharger, sCharger = big_mt[:, cols].export_names()

# Select cols and create pandas version
big_mt = big_mt[:, cols]
big_mt_pandas = big_mt.to_pandas()
```

We looked for a Python version of `skimr`, but it doesn't seem like there is one. We also tried out `pandas profiling`, but that seemed like overkill for our purpses. Finally, we decided to pipe our `datatable` into `pandas` (above) in order to use `skim_tee` on the table in the R chunk (below). It was necessary to convert to `pandas` in the Python chunk, because we couldn't figure out how to translate a `datatable` back to an R data.frame via `reticulate`. We suspect it isn't possible, and this might be the first functionality we would vote to add to `datatable`. There is already a sizeable community of `data.table` users who are used to the syntax, and might be looking for a seemless port into Python (rather than learn `pandas` directly). As `reticulate` develops, opening this door seems to make so much sense.

In the result below, we see a lot of challenges if we had hoped to build a model to predict mpg over time. 


```{r 'skimr', echo=TRUE, message=FALSE, warning=FALSE}
# Skimr
skim_tee(py$big_mt_pandas)

```

In this last chunk, we show how to select columns from the big_mt names tuple by creating the `measures` filter using regex matches for '08'. Again, this seemed more complicated than to using .SD = patterns() and we couldn't do it in line in the frame. We show the frame with the `year_filter` which we set up earlier. 


```{python 'filter-examples', echo = TRUE}
# Regex search for variable selection
measures = [x for x in big_mt.names if re.search(r'08$|year|make|model', x)]

# Print remaining cols with measures filter
print(big_mt[year_filter,  measures])

```


# Conclusion

It took us a month to get up and running with data.table, and we are still learning nuances of it now a year later, though we use it daily. We understand that there is an up-front investment in learning the syntax, because it can be confusing and because less is written about it than `dplyr` and `pandas`. The goal of this post was to try to fill the gap which we found while surfing around trying to understand how to use the library. Python datatable is promising and we are grateful for it as familiar territory as we move over to Python. We can't tell how much of our difficulty has been because the package still seems incomplete compared to the 10 year-old data.table or weakness with Python. The need to manually set variables for non-standard evaluation, to revert to pandas to accomplish many tasks or the challenges extracting and filtering data from nested columns. We have been in awe of the data.table team and can only imagine how much goes into making these libraries run as smoothly as they do. In the next post, we will continue to use the Big MT Cars data to try out `plotnine`, the Python version of `ggplot`.

